<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Federated Minimax Optimization:$$\min_x \max_y \left{f(x,y)&#x3D;\frac{1}{n}\sum_{i&#x3D;1}^{n}f_i(x,y)\right}$$  $f_i(x,y)$ is convex in $x$ while concave in $y$ Gradient decrease in $x$ while Gradie">
<meta property="og:type" content="article">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/2022/08/09/FedLearn%20-%20ICML%202022/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Federated Minimax Optimization:$$\min_x \max_y \left{f(x,y)&#x3D;\frac{1}{n}\sum_{i&#x3D;1}^{n}f_i(x,y)\right}$$  $f_i(x,y)$ is convex in $x$ while concave in $y$ Gradient decrease in $x$ while Gradie">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2022-08-09T09:47:49.207Z">
<meta property="article:modified_time" content="2022-08-11T14:21:08.990Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 6.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-FedLearn - ICML 2022" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/08/09/FedLearn%20-%20ICML%202022/" class="article-date">
  <time class="dt-published" datetime="2022-08-09T09:47:49.207Z" itemprop="datePublished">2022-08-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h3 id="Federated-Minimax-Optimization"><a href="#Federated-Minimax-Optimization" class="headerlink" title="Federated Minimax Optimization:"></a>Federated Minimax Optimization:</h3><p>$$<br>\min_x \max_y \left{f(x,y)&#x3D;\frac{1}{n}\sum_{i&#x3D;1}^{n}f_i(x,y)\right}<br>$$</p>
<ul>
<li>$f_i(x,y)$ is convex in $x$ while concave in $y$</li>
<li>Gradient decrease in $x$ while Gradient increase in $y$</li>
<li>Theoretical analysis</li>
</ul>
<hr>
<h3 id="Orchestra-Unsupervised-Federated-Learning-via-Globally-Consistent-Clustering"><a href="#Orchestra-Unsupervised-Federated-Learning-via-Globally-Consistent-Clustering" class="headerlink" title="Orchestra: Unsupervised Federated Learning via Globally Consistent Clustering"></a>Orchestra: Unsupervised Federated Learning via Globally Consistent Clustering</h3><p><em>problem</em>:</p>
<ol>
<li>Needs user interaction: Interaction parrterns from labels</li>
<li>Most sensors collect data in non-interactive manners<br><em>Derives: Need to go Unsupervised!</em><br>![[Pasted image 20220803162134.png]]<br><strong>My problem</strong>:</li>
</ol>
<ul>
<li>What’s the centroids?</li>
<li>How to conduct local unsupervised learning?</li>
</ul>
<hr>
<h4 id="An-Equivalence-Between-Data-Poisoning-and-Byzantine-Gradient-Attacks"><a href="#An-Equivalence-Between-Data-Poisoning-and-Byzantine-Gradient-Attacks" class="headerlink" title="An Equivalence Between Data Poisoning and Byzantine Gradient Attacks"></a>An Equivalence Between Data Poisoning and Byzantine Gradient Attacks</h4><ul>
<li>The <strong>central computing</strong> may suffer from the <em>Poisonous data</em></li>
<li>The <strong>Federated Learning</strong> may suffer from <em>Byzantine gradient</em><br><em>What the paper do:</em></li>
</ul>
<ol>
<li>Targetd gradient attack</li>
<li>Data poisoning and Byzantine gradients are <em>equivalent</em> (Under relistic and desirable GPL assumptions + convexity)<br><em>What I can learn:</em></li>
<li>The way to conduct different attack.</li>
<li>How to relieve the impact of the attack?</li>
</ol>
<hr>
<h4 id="Fast-Composite-Optimization-and-Statistical-Recovery-in-Federated-Learning"><a href="#Fast-Composite-Optimization-and-Statistical-Recovery-in-Federated-Learning" class="headerlink" title="Fast Composite Optimization and Statistical Recovery in Federated Learning"></a>Fast Composite Optimization and Statistical Recovery in Federated Learning</h4><p><em>Presentation is not clear enough! – Not Interested!</em></p>
<hr>
<h4 id="FedNest-Federated-Bilevel-Minimax-and-Compositional-Optimization"><a href="#FedNest-Federated-Bilevel-Minimax-and-Compositional-Optimization" class="headerlink" title="FedNest: Federated Bilevel, Minimax, and Compositional Optimization"></a>FedNest: Federated Bilevel, Minimax, and Compositional Optimization</h4><ul>
<li>Propose <strong>Federated Bilevel Optimization</strong><br><em>Application:</em><br>meta-learning, hyperparameter optimization, neural network architecture search, a-c rl,…<br><em>Form:</em><br>$$<br>\begin{gathered}<br>\min_{x\in \mathbb{R}^{d_1}} &amp;f(x) &#x3D; \frac{1}{m}\sum_{i&#x3D;1}^{m}f_i(x,y^*(x))\<br>\text{subj. to} &amp; y^*(x)\in \arg\min_{y\in \mathbb{R}^{d_2}} \frac{1}{m}\sum_{i&#x3D;1}^{m}g_i(x,y)<br>\end{gathered}<br>$$<blockquote>
<p>Not practical enough, but the definition of <strong>FBO</strong> and theorem can be transmitted to the future FL paper.</p>
</blockquote>
</li>
</ul>
<hr>
<h4 id="Deep-Neural-Network-Fusion-via-Graph-Matching-with-Applications-to-Model-Ensemble-and-Federated-Learning"><a href="#Deep-Neural-Network-Fusion-via-Graph-Matching-with-Applications-to-Model-Ensemble-and-Federated-Learning" class="headerlink" title="Deep Neural Network Fusion via Graph Matching with Applications to Model Ensemble and Federated Learning"></a>Deep Neural Network Fusion via Graph Matching with Applications to Model Ensemble and Federated Learning</h4><p><em>The Graph model is used in the combination process in the server, worth to be read when have time.</em></p>
<hr>
<h4 id="Bitwidth-Heterogeneous-Federated-Learning-with-Progressive-Weight-Dequantization"><a href="#Bitwidth-Heterogeneous-Federated-Learning-with-Progressive-Weight-Dequantization" class="headerlink" title="Bitwidth Heterogeneous Federated Learning with Progressive Weight Dequantization"></a>Bitwidth Heterogeneous Federated Learning with Progressive Weight Dequantization</h4><p><em>Problem:</em></p>
<ul>
<li>The device on edge and server may have different bit-widths for computation and stroage.</li>
<li>Due to distributional incompatibility, low-bitwidth clients cannot combined with high-bitwidth one.<br><em>Solution:</em></li>
<li><strong>Progressive Weight Dequantizer</strong>: <em>Progressively converts</em> the set of weights into a higher bitwidth.</li>
<li><strong>Selective Weight Aggregation</strong><br>$$<br>c^*&#x3D;\arg\max_c \frac{(c \cdot \Delta \hat{w}<em>{\text{Low}})^T\Delta \hat{w}</em>{\text{High}}}{\Vert c \cdot \Delta \hat{w}<em>{\text{Low}} \Vert \Vert \Delta \hat{w}</em>{\text{High}}\Vert}<br>$$<br>$$<br>w_G \leftarrow \frac{1}{N}\sum_{n&#x3D;1}^{N}c_n \cdot w_n<br>$$</li>
</ul>
<hr>
<h4 id="QSFL-A-Two-Level-Uplink-Communication-Optimization-Framework-for-Federated-Learning"><a href="#QSFL-A-Two-Level-Uplink-Communication-Optimization-Framework-for-Federated-Learning" class="headerlink" title="QSFL: A Two-Level Uplink Communication Optimization Framework for Federated Learning"></a>QSFL: A Two-Level Uplink Communication Optimization Framework for Federated Learning</h4><blockquote>
<p>FL is a paradigm for collaboratively training a share model with privacy protection, It allows clients only exchange parameter&#x2F;gradint updates with the server, and data is always stored locally. #Definition</p>
</blockquote>
<p><em>Problem</em>:<br><strong>Communication cost</strong></p>
<ol>
<li>Uplink transmission reach TB-level</li>
<li>Expensive&#x2F;Unreliable wireless connections</li>
<li>Redundant Parameters</li>
<li>Uploading speed is slower than the downloading speed.<br><em>Solutions:</em></li>
<li>Client-Level: Qualification Judgment: Selecting high-quality model upload to the server</li>
<li>Model-Lever: Sparse Cyclic Sliding Segment</li>
</ol>
<p><em>Solution 1:</em><br>$$<br>\begin{gather}<br>q_k&amp;&#x3D;\beta \cdot \text{contribution}_k + (1+\beta)\cdot\text{relevance}<em>k\<br>&amp;&#x3D;\beta\cdot \frac{\text{loss}<em>k&#x2F;n_k}{\sum</em>{i&#x3D;1}^C\text{loss}<em>i&#x2F;n_i}\<br>&amp;+(1-\beta)\cdot\frac{\sum</em>{j&#x3D;1}^{|\omega_k^l|}\mathbb{I}(\text{sgn}(\omega</em>{k,j}^l)&#x3D;&#x3D;\text{sgn}(\omega_j^g))}{|\omega_k^l|}<br>\end{gather}<br>$$<br>My Comment: Easy to understand, the contribution evaluate the performance of the client from the view of whole picture. (<em>So Should Choose the one with lager loss?</em>). Then, the relevance is computed through the comparing parameters from the clients to the server. Note that the client with higher relevance would be considered approaches more to the global model, thus should be selected with higher probability.</p>
<p><em>Solution 2:</em><br>![[Pasted image 20220811220826.png]]<br>Cycleing upload the parameters to the server, as shown in the above figure.</p>
<hr>
<h4 id="DAdaQuant-Doubly-adaptive-quantization-for-communication-efficient-Federated-Learning"><a href="#DAdaQuant-Doubly-adaptive-quantization-for-communication-efficient-Federated-Learning" class="headerlink" title="DAdaQuant: Doubly-adaptive quantization for communication-efficient Federated Learning"></a>DAdaQuant: Doubly-adaptive quantization for communication-efficient Federated Learning</h4><p>Simply saying:<br>    This method combines both QSGD[^Means] and a method to select $q$ according to the loss function of each client.</p>
<p>[^Means]: Quantization SGD, which is designed through two concepts: 1. Making random quantization with original statistical characteristic, and 2. Partical loosy encoding to the integral part of the quantized gradient. <a target="_blank" rel="noopener" href="https://www.cnblogs.com/mhlan/p/15982260.html">【论文考古】量化SGD QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding - 木坑 - 博客园 (cnblogs.com)</a></p>
<hr>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/08/09/FedLearn%20-%20ICML%202022/" data-id="cl76ayuf40000cgu9f9fag6zn" data-title="" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2022/08/10/%E7%AB%9E%E8%B5%9B/%E5%8D%97%E7%BD%91-%E9%A3%8E%E5%85%89%E5%87%BA%E5%8A%9B%E9%A2%84%E6%B5%8B/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          (no title)
        
      </div>
    </a>
  
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/08/">August 2022</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/08/22/IELTS/Listening/Chapter-III-Test-I/">(no title)</a>
          </li>
        
          <li>
            <a href="/2022/08/22/IELTS/Listening/Chacpter-III-FirstTime/">(no title)</a>
          </li>
        
          <li>
            <a href="/2022/08/20/IELTS/Oral/%E6%9C%AA%E5%91%BD%E5%90%8D/">(no title)</a>
          </li>
        
          <li>
            <a href="/2022/08/17/%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87/S3-%E5%A4%9A%E5%BE%AE%E7%BD%91%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0/%E5%BA%94%E7%94%A8%E5%A4%8F%E6%99%AE%E5%88%A9%E5%80%BC%E7%9A%84%E8%81%94%E9%82%A6%E8%B4%A1%E7%8C%AE%E5%BA%A6%E5%88%86%E9%85%8D/">(no title)</a>
          </li>
        
          <li>
            <a href="/2022/08/17/%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87/S1-%E9%A3%8E%E5%85%89%E6%A6%82%E7%8E%87%E9%A2%84%E6%B5%8B/%E9%A3%8E%E5%85%89%E5%87%BA%E5%8A%9B%E6%A6%82%E7%8E%87%E9%A2%84%E6%B5%8B/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2022 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>